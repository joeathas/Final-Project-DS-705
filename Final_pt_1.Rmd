---
title: "Final Project DS 705"
author: "Joe Athas"
output: word_document
---

```{r include=FALSE}
options(digits = 4)
options(max.print = 200)
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)

library(dplyr)
library(plyr)
library(moments)
library(ggplot2)
library(psych)
library(leaps)
library(scales)
```
#Introduction
It is very important banks only give loans to those that are able to pay them back in full. Loans without a lien can be a potential risk for a bank. Loans that have a lien or collateral are also very costly in the event of a default. The goal for this data exploration and analysis is to determine a model with appropriate predictor variables using data often gathered by financial institutions and to use that model to predict the likelihood of a loan applicant defaulting or paying off the loan. To do this some manipulation of the data may be in order. I will be using R to do preliminary exploration, cleaning, and transformation of the provided sample data, “loans50k.csv”. 
```{r}
citation()
```
#Section 3 Preparing and Cleaning the data
Upon receiving the data, the first thing I did was make the appropriate response variable: A binary variable (as is needed for logistic regression) produced from the original variable “status”. Fully paid loans were reclassified as good, those in “default” or “charged off” were reclassified as bad. All other levels of status (loans in progress or not complete) were removed from the dataset. 

```{r}
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
  loans50k %>%
  mutate(payment_status = case_when(
    status == "Fully Paid" ~ "Good", 
    status == "Charged Off" ~ "Bad",
     status == "Default" ~ "Bad",
    TRUE ~ ""
  ))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)

#change the empty string cells to NAs
#Turning any fields into NA values makes it easier to eliminate later
df[df==""]<-NA
```

I also removed “employed” immediately as there were too many levels and empty values. Last thing I did before really digging into the data was removing all the rows containing NA values. All the remaining NAs were in I noticed it only reduced n by approximately 1% which didn’t compromise the integrity of the original set all too much. A few other variables were removed for redundancy. 
```{r results='hide'}
#Test where all NAs are to 
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.  
df$employment<- NULL

#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries, 
#or roughly 1% of the dataset. Eliminating these wont be a huge issue. 

#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
```
Here a covariance matrix was constructed of the numerical predictors to aid in this process of determining redundancy. I chose an arbitrary value of .8 of Pearson’s r to classify variables as strongly correlated. The strong correlations were as follows: Total limit of credit cards was heavily correlated with Total unused credit on cards. Unused credit (bcOpen) seemed like a more logical choice than totalBcLim (total limit of cc) and it eliminated some redundancy. ‘Payment’ was heavily associated with ‘amount’ as monthly payment is a function of the total amount of the loan. I deleted this variable to eliminate redundancy. Proportion of revolving credit use(revolRatio) was eliminated due to redundancy with ratio of total credit card limits (bcOpen). Average balance and total credit limit were both strongly correlated with total Balance. I figured there is a lot of redundancy between these three variables, keeping just totalBal would suffice.
```{r}
#make subset of numerical data check covariance matrix on numeric factors. 
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
#Found few redundant variables...will eliminate
```
A few predictor variables were initially removed due to lack of relevance. These included “totalPaid”, “state”, and “loanID”. “State” and “loanID” have nothing really to do with the actual loan, whereas “totalpaid” cannot be a predictor variable of a completed loan.  A few more were eliminated based on redundancy as mentioned above. 
```{r}
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character) 
sapply((sapply(dfChar,unique)),length)

cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))


```
A few other variables I touched up were “verified” and “reason”. “Verified” had three levels, two of which were redundant; “source verified” and “verified”. I changed all “source verified” to just “verified” to eliminate the mistake. “Reason” contained a few levels with very few instances which is considered less powerful for logistic regression. I modified “wedding” and “renewable_energy” to “other” and “house” to “major_purchase” to consolidate a few levels. After going about data transformation in a later step, a few more concerning variables were mutated. ‘pubRec’ and ‘delinq2yr’ (number of derogatory public records and number of 30+ day late payments in last two years respectively) both contained many instances of 0 due to their nature. Since many loans have ‘0’ represented in these respective variables, choosing to set it as categorical made more sense. I chose three levels for both variables (the same since they were similar scales) “None”, “1-3” and “More than 3”. Instructions
```{r results='hide'}
#consolidate "verified" variable into two levels instead of three
df_New$verified<-revalue(df_New$verified, c("Source Verified"="Verified"))
unique(df1$verified)

#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))

#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
```
###After removing redundencies and a few predictor variables, initial cleaning of dataset brings n = 34271.

#Section 4 - “Exploring and Transforming the data” 
Certain data transformations were necessary in order to meet the assumption of normality for logistic regression. I first analyzed the skewness of each distribution. Considering a skewness greater than the absolute value of 1 is considered “highly skewed”, I created a r function to attempt various transformations, specifically a log, natural log, square root, cube root, 4th root, and reciprocal, then return the skewness of the transformed distribution. I applied this function to every variable with an initial skewness greater than the absolute value of 1 and determined which transformation technique was the most successful at reducing skewness. Obviously this is a blanket technique, and eaach variable still needs to be looked at individually. 
After determining which transformation to use for each variable, I performed the transformations as follows
income: log transformation 
openAcc: log + 1 transformation 
totalBal: cubed root transformation 
totalRevLim: cubed root transformation 
accOpen24: square root transformation 
bcOpen: cube root transformation 
totalRevBal: cube root transformation


```{r results='hide'}
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)

#check out skewness of all numeric variables
par(mfrow=c(3,4))
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal

```

```{r}
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
  log=skew(log(dist1))
  lognat=skew(log(dist1,base=exp(1)))
  sqrt=skew(sqrt(dist1))
  curt=skew(sign(dist1) * abs(dist1)^(1/3) )
  fthrt=skew(sign(dist1)^(1/4))
  reciprical=skew(1/dist1)
  
  list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
  
  return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)

#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both. 

#use unique() to determine breaks
unique(df_New$pubRec)

#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$pubRec)<-c("None","1-3","More than 3")

unique(df_New$pubRec)
#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
```

```{r message=FALSE,results='hide'}

#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc+1) #add 1 to oppenAcc to avoid taking log(0)
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)

#Check if any NA values induced
which(is.na(df_New), arr.ind=TRUE)
df_New<-na.omit(df_New)


#make new data frame to test skew on numeric data to see if successful.
dfNum1<-select_if(df_New, is.numeric)

```

```{r echo=T, results='hide',fig.show = 'hide'}
#Double check new histograms
par(mfrow=c(3,3))
sapply(dfNum1,hist)
#Double check new skew
sapply(dfNum1,skew)
```
Plots and skew are looking more normal which passes the assumptions of logistic regression.

#Exploration: 
Side-by-side boxplots for continuous data and frequency tables for categorical data were created to compare each factor to the dependent variable. Few variables popped out immediately as good indicators, however, few were found. The variable “term” has a much higher ratio of bad loans to good in the loans that are 60 months vs 36 months. Loans rated ‘D’ or lower also have a much higher ratio of being “bad” than their counterparts.
```{r echo= FALSE}
par(mfrow=c(3,4))

#see visual differences if any of good loans vs bad
boxplot(income~payment_status,data=df_New)
boxplot(amount~payment_status,data=df_New)
boxplot(debtIncRat~payment_status,data=df_New)
boxplot(openAcc~payment_status,data=df_New)
boxplot(totalAcc~payment_status,data=df_New)
boxplot(totalBal~payment_status,data=df_New)
boxplot(totalRevLim~payment_status,data=df_New)
boxplot(accOpen24~payment_status,data=df_New)
boxplot(bcOpen~payment_status,data=df_New)
boxplot(totalRevBal~payment_status,data=df_New)
```

```{r include=FALSE}
#compare frequency tables
getFreqTable<-function(var1){
  mytable <- table(var1,df_New$payment_status) 
  return(ftable(mytable))
}
getFreqTable(df_New$term)
getFreqTable(df_New$grade)
getFreqTable(df_New$length)
getFreqTable(df_New$home)
getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
getFreqTable(df_New$delinq2yr)
```
##Splitting the data into test and trainig data. 
```{r}
## 80% of the sample size
smp_size <- floor(0.8 * nrow(df_New))
## set the seed to make partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df_New)), size = smp_size)

train <- df_New[train_ind, ]
test <- df_New[-train_ind, ]

#eliminated totalPaid as variable
train$totalPaid<-NULL

#Coerce "payment_status" response variable into factor to perform logistic regression. 
train$payment_status <- as.factor(train$payment_status)
```
#Begin Logistic Regression Exploration
Using forward stepwise selection in R, a few more predictors were eliminated based on AIC.  
```{r results='hide'}

full_model2<-glm(payment_status~.,data=train,family="binomial")
null_model2 <- glm(payment_status~1,data=train,family="binomial")
#step(null_model2,scope=list(lower=null_model2,upper=full_model2),direction="forward")
```
The new model chosen via forward stepwise selection (AIC as selection criteria) is represented by 'newModel' below.
```{r results='hide'}
newModel<-glm(formula = payment_status ~ grade + term + debtIncRat + totalBal + accOpen24 + bcOpen + reason + amount + length + home + totalAcc +   delinq2yr + inq6mth + income + rate + openAcc + totalRevLim + verified, family = "binomial", data = train)
summary(newModel)

```
After examining a full model there are many statistically significant predictors.  Eliminating a few would likely be benificial to reduce the potential for multicoliniarity in addition to impose some simplicity. Using the regsubsets function from the 'Leaps' package allows comparing many potential models using the full model.  

#Use regsubsets to find best model based on $R^2_{adj}$
```{r results= 'hide'}
all_models <- regsubsets(payment_status~.,nvmax=12,data=train)
summary(all_models)
summary(all_models)$adjr2
```
After cross examining a few of the models created by the regsubsets function in r and the 'best' model based on stepwise selection, a few more predictors were eliminated. The ones deemed worth of keeping for now are represented by the formula below.  
```{r}
modelA<-glm(formula = payment_status ~ grade + term + debtIncRat + totalBal + 
    accOpen24 + bcOpen  + amount  + home + totalAcc +  rate,data = train,family = "binomial")
summary(modelA)
```
After the model was fit, the testing data was input to the model to predict probabilites of loan status. Below is a contingency table and the given proportions of correctly predicted loans both good and bad. 
```{r echo=F}
predprob <- predict(modelA, test, type="response") # get predicted probabilities

threshhold <- 0.68
predLoan <- cut(predprob, breaks=c(-Inf, threshhold, Inf), 
                labels=c("Bad Loan", "Good Loan"))  # Y=1 is "Good" here
cTab <- table(test$payment_status, predLoan) 
addmargins(cTab)

p <- sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
badLoanp<-cTab[1]/(cTab[1]+cTab[2])
goodLoanp<-cTab[4]/(cTab[4]+cTab[3])

print(paste('Proportion correctly predicted = ', p)) 
print(paste('Proportion of correctly predicted Bad Loans =',badLoanp))
print(paste('Proportion of correctly predicted Good Loans =',goodLoanp))

```
Although our "Good" loans have a pretty high success rate at 78.2%, the "bad" ones are a little less accurate coming in at a success rate of 54.8%.  Here is where a little intuition helps; a bank doesn't care about predicting good loans as much as it does bad ones. This model is more helpful than chance alone, but the risks are still pretty high due to the inability to properly predict more than 45% of the bad loans. In other words, aproximately 20% of the loans predicted as good will actually be bad, whereas 45% of the loans predicted as bad could have actually been good. At this point I wouldn't use this model to actually predict loans. 
```{r echo= FALSE}
threshhold3 <- 0.3

predLoan <- cut(predprob, breaks=c(-Inf, threshhold3, Inf), 
                labels=c("Bad Loan", "Good Loan"))  # Y=1 is "Good" here

cTab3 <- table(test$payment_status, predLoan)

p3 <- sum(diag(cTab3)) / sum(cTab3)  
badLoanp3<-cTab3[1]/(cTab3[1]+cTab3[2])
goodLoanp3<-cTab3[4]/(cTab3[4]+cTab3[3])
```

```{r echo= FALSE}
threshhold4 <- 0.4

predLoan <- cut(predprob, breaks=c(-Inf, threshhold4, Inf), 
                labels=c("Bad Loan", "Good Loan"))  # Y=1 is "Good" here

cTab4 <- table(test$payment_status, predLoan) 

p4 <- sum(diag(cTab4)) / sum(cTab4)  
badLoanp4<-round(cTab4[1]/(cTab4[1]+cTab4[2]),3)
goodLoanp4<-round(cTab4[4]/(cTab4[4]+cTab4[3]),3)

```

```{r echo=FALSE}
threshhold35 <- 0.35

predLoan <- cut(predprob, breaks=c(-Inf, threshhold35, Inf), 
                labels=c("Bad Loan", "Good Loan"))  # Y=1 is "Good" here

cTab35 <- table(test$payment_status, predLoan)

p35 <- sum(diag(cTab35)) / sum(cTab35)  
badLoanp35<-round(cTab35[1]/(cTab35[1]+cTab35[2]),3)
goodLoanp35<-round(cTab35[4]/(cTab35[4]+cTab35[3]),3)

```

```{r echo=FALSE}
threshhold6 <- 0.6

predLoan <- cut(predprob, breaks=c(-Inf, threshhold6, Inf), 
                labels=c("Bad Loan", "Good Loan"))  # Y=1 is "Good" here

cTab6 <- table(test$payment_status, predLoan)

p6 <- sum(diag(cTab6)) / sum(cTab6)  
badLoanp6<-round(cTab6[1]/(cTab6[1]+cTab6[2]),3)
goodLoanp6<-round(cTab6[4]/(cTab6[4]+cTab6[3]),3)

```
After playing around with the threshold of the predict() function, I saved a few bar plots of the contingency talbes of various thresholds.  Below are a few of the graphs for comparison.

```{r echo= FALSE, fig.height = 8}
par(mfrow=c(3,1))
barplot(cTab,main = paste("Predicted propabilities with Threshold =",threshhold),sub = paste("Bad Loans Success Rate =",round(badLoanp,3),"Good Loans Success Rate =",goodLoanp))
barplot(cTab3,main = paste("Predicted propabilities with Threshold =",threshhold3),sub = paste("Bad Loans Success Rate =",round(badLoanp3,3),"Good Loans Success Rate =",goodLoanp3))
barplot(cTab35,main = paste("Predicted propabilities with Threshold =",threshhold35),sub = paste("Bad Loans Success Rate =",round(badLoanp35,3),"Good Loans Success Rate =",goodLoanp35))
barplot(cTab4,main = paste("Predicted propabilities with Threshold =",threshhold4),sub = paste("Bad Loans Success Rate =",round(badLoanp4,3),"Good Loans Success Rate =",goodLoanp4))
barplot(cTab6,main = paste("Predicted propabilities with Threshold =",threshhold6),sub = paste("Bad Loans Success Rate =",round(badLoanp6,3),"Good Loans Success Rate =",goodLoanp6))
```

It appears the lower the threshhold, the more loans are predicted as 'Good'.  Overall accuracy is pretty consistent from a threshhold of .3 to .6. Accuracy maximizes around .52.
```{r}
#Join predprobs with test data
test1<-cbind(test, predprob)
```
Here is a function to produce profits given a threshold for predition probability. Using this function and modifying the threshold in the parameter allows comparison of profit return.  
```{r}
get_profits<-function(threshhold,test1){
  new_data <- subset(test1, predprob > threshhold)
  profits<-new_data$totalPaid-new_data$amount
  x<-sum(profits)
  return(x)
}
get_profits(.68,test1)
get_profits(.6803 ,test1)
get_profits(.6807,test1)

```
After some trial and error, the threshhold that returns the max profit is approximately .6803. The profits for the test data set with a threshhold of .6803 are around $3,497,823.

```{r echo=F}
perfectSubset<-subset(test,payment_status=="Good")
profitPerfect<-perfectSubset$totalPaid-perfectSubset$amount
print(paste("If we could predict every loan considered 'Good' with 100% accuracy, profits are", dollar(sum(profitPerfect))))
profitAggregate<-test$totalPaid-test$amount
print(paste("If  every loan was accepted, good or bad, profits are only", dollar(sum(profitAggregate))))
sum(profitAggregate)/sum(profitPerfect)

```


The hypothetical perfect model would be by far the most profitable weighing in at \$12,365,881. Using no model, and simply accepting every loan equates to \$1,457,687 in profit which equates to 11.79% of the profits.

Using the best logistic model above represented by modelA and a threshold of .6803, profits are around $3,497,823, or 28.29% the perfect predictor. Of course it is still impossible to perfectly predict whether or not a person will default on a loan. However, using the model we can accept more laons and more than double profits than simply accepting every loan. 

Interestingly enough the threshhold used for categorizing Good vs Bad loans based on based on profit maximization(.68) is less accurate than the threshholds maximizing accuracy (.52). However, not by a significant margin (.781 and .742 respectively.) 


#Results Summary
Using logistic regression, we can more accurately predict whether or not a loan will fail than chance alone. In addition to this, we can maximize profits using a logistic model.  The following predictor variables were chosen as after redundancy was eliminated and those only deemed statistically significant: grade, term, debtIncRat, totalBal, accOpen24, bcOpen, amount, home, and totalAcc.
The classification threshold chosen to maximize profits was .6803.  This model predicted loan classification with 74% accuracy. The model also is deemed more profitable than accepting every loan outright.  Room could potentially be made for improvement, the $R^2$ for this model only weighed in around .10. 90% of variance is unaccounted for using these variables alone, other missing variables might provide additional insight. 

