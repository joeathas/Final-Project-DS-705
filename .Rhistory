getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
getFreqTable(df_New$delinq2yr)
#Eventually...
mylogit <- glm(as.factor(payment_status) ~ ., data = df_New, family = "binomial")
citation()
?citation()
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(plyr)
library(dplyr)
library(moments)
library(ggplot2)
library(psych)
#consolidate "verified" variable into two levels instead of three
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(plyr)
library(dplyr)
library(moments)
library(ggplot2)
library(psych)
citation()
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
#Found few redundant variables...will eliminate
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#consolidate "verified" variable into two levels instead of three
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
options(max.print = 20)
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(plyr)
library(dplyr)
library(moments)
library(ggplot2)
library(psych)
sapply(dfNum,skewness)
#check out skewness of all numeric variables
sapply(dfNum, hist)
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
#check out skewness of all numeric variables
par(mfrow=c(4,4))
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
#check out skewness of all numeric variables
par(mfrow=c(3,2))
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
View(df_New)
#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc+1) #
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)
which(is.na(df_New), arr.ind=TRUE)
#make new data frame to test skew on numeric data to see if successful.
dfNum1<-select_if(df_New, is.numeric)
sapply(dfNum1,skew)
par(mfrow=c(2,2))
sapply(dfNum1,hist)
which(is.na(df_New), arr.ind=TRUE)
options(max.print = 20)
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(dplyr)
library(plyr)
library(moments)
library(ggplot2)
library(psych)
citation()
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
#Found few redundant variables...will eliminate
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#consolidate "verified" variable into two levels instead of three
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
#check out skewness of all numeric variables
par(mfrow=c(3,2))
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
#use unique() to determine breaks
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$pubRec)<-c("None","1-3","More than 3")
unique(df_New$pubRec)
#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc+1) #
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)
which(is.na(df_New), arr.ind=TRUE)
#make new data frame to test skew on numeric data to see if successful.
dfNum1<-select_if(df_New, is.numeric)
sapply(dfNum1,skew)
par(mfrow=c(2,2))
sapply(dfNum1,hist)
par(mfrow=c(2,3))
#see visual differences if any of good loans vs bad
boxplot(income~payment_status,data=df_New)
boxplot(amount~payment_status,data=df_New)
boxplot(debtIncRat~payment_status,data=df_New)
boxplot(openAcc~payment_status,data=df_New)
boxplot(totalAcc~payment_status,data=df_New)
boxplot(totalBal~payment_status,data=df_New)
boxplot(totalRevLim~payment_status,data=df_New)
boxplot(accOpen24~payment_status,data=df_New)
boxplot(bcOpen~payment_status,data=df_New)
boxplot(totalRevBal~payment_status,data=df_New)
#compare frequency tables
getFreqTable<-function(var1){
mytable <- table(var1,df_New$payment_status)
return(ftable(mytable))
}
getFreqTable(df_New$term)
getFreqTable(df_New$grade)
getFreqTable(df_New$length)
getFreqTable(df_New$home)
getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
getFreqTable(df_New$delinq2yr)
#Eventually...
mylogit <- glm(as.factor(payment_status) ~ ., data = df_New, family = "binomial")
which(is.na(df_New), arr.ind=TRUE)
which(is.na(df_New), arr.ind=TRUE)
options(max.print = 1000)
which(is.na(df_New), arr.ind=TRUE)
df_New<-na.omit(df_New)
options(max.print = 20)
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(dplyr)
library(plyr)
library(moments)
library(ggplot2)
library(psych)
citation()
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
#Found few redundant variables...will eliminate
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#consolidate "verified" variable into two levels instead of three
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
#check out skewness of all numeric variables
par(mfrow=c(3,2))
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
#use unique() to determine breaks
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$pubRec)<-c("None","1-3","More than 3")
unique(df_New$pubRec)
#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc+1) #
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)
#Check if any NA values induced
which(is.na(df_New), arr.ind=TRUE)
df_New<-na.omit(df_New)
#make new data frame to test skew on numeric data to see if successful.
dfNum1<-select_if(df_New, is.numeric)
sapply(dfNum1,skew)
par(mfrow=c(2,2))
sapply(dfNum1,hist)
par(mfrow=c(2,3))
#see visual differences if any of good loans vs bad
boxplot(income~payment_status,data=df_New)
boxplot(amount~payment_status,data=df_New)
boxplot(debtIncRat~payment_status,data=df_New)
boxplot(openAcc~payment_status,data=df_New)
boxplot(totalAcc~payment_status,data=df_New)
boxplot(totalBal~payment_status,data=df_New)
boxplot(totalRevLim~payment_status,data=df_New)
boxplot(accOpen24~payment_status,data=df_New)
boxplot(bcOpen~payment_status,data=df_New)
boxplot(totalRevBal~payment_status,data=df_New)
#compare frequency tables
getFreqTable<-function(var1){
mytable <- table(var1,df_New$payment_status)
return(ftable(mytable))
}
getFreqTable(df_New$term)
getFreqTable(df_New$grade)
getFreqTable(df_New$length)
getFreqTable(df_New$home)
getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
getFreqTable(df_New$delinq2yr)
#Eventually...
mylogit <- glm(as.factor(payment_status) ~ ., data = df_New, family = "binomial")
#Eventually...
mylogit <- glm(as.factor(payment_status) ~ ., data = df_New, family = "binomial")
summary(mylogit)
options(max.print = 1000)
#Eventually...
mylogit <- glm(as.factor(payment_status) ~ ., data = df_New, family = "binomial")
summary(mylogit)
options(max.print = 1000)
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#consolidate "verified" variable into two levels instead of three
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
#check out skewness of all numeric variables
par(mfrow=c(3,2))
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
#use unique() to determine breaks
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$pubRec)<-c("None","1-3","More than 3")
unique(df_New$pubRec)
#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc+1) #add 1 to oppenAcc to avoid taking log(0)
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)
#Check if any NA values induced
which(is.na(df_New), arr.ind=TRUE)
df_New<-na.omit(df_New)
#make new data frame to test skew on numeric data to see if successful.
dfNum1<-select_if(df_New, is.numeric)
sapply(dfNum1,skew)
par(mfrow=c(2,2))
sapply(dfNum1,hist)
par(mfrow=c(2,3))
#see visual differences if any of good loans vs bad
boxplot(income~payment_status,data=df_New)
boxplot(amount~payment_status,data=df_New)
boxplot(debtIncRat~payment_status,data=df_New)
boxplot(openAcc~payment_status,data=df_New)
boxplot(totalAcc~payment_status,data=df_New)
boxplot(totalBal~payment_status,data=df_New)
boxplot(totalRevLim~payment_status,data=df_New)
boxplot(accOpen24~payment_status,data=df_New)
boxplot(bcOpen~payment_status,data=df_New)
boxplot(totalRevBal~payment_status,data=df_New)
#compare frequency tables
getFreqTable<-function(var1){
mytable <- table(var1,df_New$payment_status)
return(ftable(mytable))
}
getFreqTable(df_New$term)
getFreqTable(df_New$grade)
getFreqTable(df_New$length)
getFreqTable(df_New$home)
getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
getFreqTable(df_New$delinq2yr)
#Eventually...
mylogit <- glm(as.factor(payment_status) ~ ., data = df_New, family = "binomial")
summary(mylogit)
options(max.print = 1000)
## 80% of the sample size
smp_size <- floor(0.8 * nrow(df_New))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df_New)), size = smp_size)
train <- df_New[train_ind, ]
test <- df_New[-train_ind, ]
View(train)
train$totalPaid<-NULL
View(train)
