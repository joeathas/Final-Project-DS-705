#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
unique(df_New$delinq2yr)
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$PubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$cutPubRec)<-c("None","1-3","More than 3")
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$cutPubRec)<-c("None","1-3","More than 3")
levels(df_New$pubRec)<-c("None","1-3","More than 3")
#use unique() to determine breaks
unique(df_New$pubRec)
unique(df_New$pubRec)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc)
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(dplyr)
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
TRUE ~ ""
))
#check total
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#Row 34806 is missing all values, eliminate and test again:
df <- df[-c(34806),]
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#consolidate "verified" variable into two levels instead of three
library(plyr)
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
dfNum$totalIlLim
#Found few redundant variables...will eliminate:
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
library(moments)
library(ggplot2)
#check out skewness of all numeric variables
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
library(psych)
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
boxplot(df_New$pubRec)
#use unique() to determine breaks
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$pubRec)<-c("None","1-3","More than 3")
unique(df_New$pubRec)
#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc)
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#Row 34806 is missing all values, eliminate and test again:
df <- df[-c(34806),]
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#consolidate "verified" variable into two levels instead of three
library(plyr)
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
dfNum1<-select_if(df_New, is.numeric)
sapply(dfNum1,skew)
sapply(df1,skew)
sapply(dfNum,skew)
sapply(dfNum1,skew)
unique(loans50k$status)
library(tidyr)
library(dplyr)
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad"
status == "Default" ~ "Bad",
library(tidyr)
library(dplyr)
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(dplyr)
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#consolidate "verified" variable into two levels instead of three
library(plyr)
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
unique(df1$verified)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
dfNum$totalIlLim
#Found few redundant variables...will eliminate:
View(strongCorrelations)
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
library(moments)
library(ggplot2)
#check out skewness of all numeric variables
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
library(psych)
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
boxplot(df_New$pubRec)
library(psych)
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
#use unique() to determine breaks
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
unique(df_New$pubRec)
#use unique() to determine breaks
unique(df_New$pubRec)
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
library(moments)
library(ggplot2)
#check out skewness of all numeric variables
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(dplyr)
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#consolidate "verified" variable into two levels instead of three
library(plyr)
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
dfNum$totalIlLim
#Found few redundant variables...will eliminate:
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#Load Data
library(readr)
loans50k <- read_csv("loans50k.csv")
library(tidyr)
library(dplyr)
#Make new variable: Set "Good" and "Bad" Loans, all else "blank" to be removed later
df <-
loans50k %>%
mutate(payment_status = case_when(
status == "Fully Paid" ~ "Good",
status == "Charged Off" ~ "Bad",
status == "Default" ~ "Bad",
TRUE ~ ""
))
#Probably best to get rid of the null values we aren't interested in payment_status:
df<-df %>% drop_na(payment_status)
#change the empty string cells to NAs
df[df==""]<-NA
#Test where all NAs are to
#which(is.na(df), arr.ind=TRUE)
#Found a bunch in "employment". Going to eliminate on account of too many levels/NA values.
df$employment<- NULL
#Test where the NAs are again:
which(is.na(df), arr.ind=TRUE)
#last NA values are in bcOpen and bcRatio, of which there are roughtly 400 incomplete entries,
#or roughly 1% of the dataset. Eliminating these wont be a huge issue.
#Finally omit all the rows containing NA values. (This will not eliminate the NA values in "length" as they are strings)
df1<-na.omit(df)
#consolidate "verified" variable into two levels instead of three
library(plyr)
df1$verified<-revalue(df1$verified, c("Source Verified"="Verified"))
unique(df1$verified)
#make subset of numerical data check covariance matrix on numeric factors.
dfNum<-select_if(df1, is.numeric)
m<-cor(dfNum)
#easy way to find strong correlations; arbitrary value set at .8
strongCorrelations = m > .8
dfNum$totalIlLim
#Found few redundant variables...will eliminate:
#Final check for number of levels in each categorical variable
dfChar<-select_if(df1,is.character)
sapply((sapply(dfChar,unique)),length)
cor(df$avgBal,df$totalLim)
#Eliminating: totalPaid, employment (already done), payment,revolRatio, avgBal, totalLim,state,status,loanID
df_New = as.data.frame(subset(df1, select = -c(totalPaid, payment,revolRatio, avgBal, totalLim,state,status,loanID,totalIlLim,totalBcLim)))
#going to combine a few levels in "reason" due to the lack of instances
df_New$reason<-revalue(df_New$reason,c("wedding"="other","renewable_energy"="other","house"="major_purchase"))
#make df of only continuous variables
dfCont<-select_if(df_New, is.character)
#Test for instances of each level
x=sapply(dfCont,count)
x
#lowest #of instances is now above 200
#make new df of only numeric variables.
dfNum<-select_if(df_New, is.numeric)
library(moments)
library(ggplot2)
#check out skewness of all numeric variables
sapply(dfNum, hist)
sapply(dfNum,skewness)
#Distributions that have a skewness greater than 1 are generally considered "highly skewed"
#will transform: income, delinq2yr,inq6mth,openAcc,pubRec,totalBal,totalRevLim,accOpen24,bcOpen,totalRevBal
library(psych)
#function to success of various transformations
tansformAndGetSkew<-function(dist1){
log=skew(log(dist1))
lognat=skew(log(dist1,base=exp(1)))
sqrt=skew(sqrt(dist1))
curt=skew(sign(dist1) * abs(dist1)^(1/3) )
fthrt=skew(sign(dist1)^(1/4))
reciprical=skew(1/dist1)
list1<- c(log,lognat,sqrt,curt,fthrt,reciprical)
return(list1)
}
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
#Use results to infer which might be the best type of transformation
#Changing pubRec & delinqyr to categorical variables due to too many instances of '0' in both.
#use unique() to determine breaks
unique(df_New$pubRec)
#Make three levels, one containing zero, 1-3, and more than 3
df_New$pubRec<-cut(df_New$pubRec,breaks=c(-.10,.1,3,14))
levels(df_New$pubRec)<-c("None","1-3","More than 3")
unique(df_New$pubRec)
#DO THIS AGAIN But with delinqyr
df_New$delinq2yr<-cut(df_New$delinq2yr,breaks=c(-.10,.1,3,12))
levels(df_New$delinq2yr)<-c("None","1-3","More than 3")
#begin transformations
df_New$income<-log(df_New$income)
df_New$inq6mth<-(df_New$inq6mth)^(1/4)
df_New$openAcc<-log(df_New$openAcc)
df_New$totalBal<-(df_New$totalBal)^(1/3)
df_New$totalRevLim<-(df_New$totalRevLim)^(1/3)
df_New$accOpen24<-sqrt(df_New$accOpen24)
df_New$bcOpen<-(df_New$bcOpen)^(1/3)
df_New$totalRevBal<-df_New$totalRevBal^(1/3)
#make new data frame to test skew on numeric data to see if successful.
dfNum1<-select_if(df_New, is.numeric)
sapply(dfNum1,skew)
sapply(dfNum1,skew)
dfNum$totalIlLim
sapply(dfNum1,skew)
sapply(dfNum1,hist)
sapply(dfNum1,skew)
sapply(dfNum1,hist)
sapply(dfNum1,skew)
hist(dfNum1$totalAcc)
sapply(dfNum1,skew)
sapply(dfNum1,hist)
sapply(dfNum1,skew)
hist(dfNum1$inq6mth)
sapply(dfNum,skew)
#apply function to all numerical variables
sapply(dfNum, tansformAndGetSkew)
sapply(dfNum1,skew)
boxplot(income~payment_status,data=df_New)
View(df_New)
boxplot(amount~payment_status,data=df_New)
#see visual differences if any of good loans vs bad
boxplot(income~payment_status,data=df_New)
boxplot(amount~payment_status,data=df_New)
boxplot(debtIncRat~payment_status,data=df_New)
boxplot(openAcc~payment_status,data=df_New)
boxplot(totalAcc~payment_status,data=df_New)
boxplot(totalBal~payment_status,data=df_New)
boxplot(totalRevLim~payment_status,data=df_New)
boxplot(accOpen24~payment_status,data=df_New)
boxplot(bcOpen~payment_status,data=df_New)
boxplot(totalRevBal~payment_status,data=df_New)
mytable <- table(df_New$grade,df_New$payment_status)
ftable(mytable)
mytableTr <- table(df_New$term,df_New$payment_status)
ftable(mytableTr)
#compare
getFreqTable<-function(var1){
mytable <- table(var1,df_New$payment_status)
return(ftable(mytable))
}
getFreqTable(df_New$term)
getFreqTable(df_New$term)
getFreqTable(df_New$grade)
getFreqTable(df_New$length)
getFreqTable(df_New$verified)
barplot(getFreqTable(df_New$delinq2yr))
barplot(getFreqTable(df_New$delinq2yr))
getFreqTable(df_New$delinq2yr)
getFreqTable(df_New$delinq2yr)
getFreqTable(df_New$reason)
getFreqTable(df_New$reason)
getFreqTable(df_New$home)
getFreqTable(df_New$home)
getFreqTable(df_New$term)
getFreqTable(df_New$term)
getFreqTable(df_New$grade)
getFreqTable(df_New$length)
getFreqTable(df_New$home)
getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
getFreqTable(df_New$grade)
getFreqTable(df_New$term)
getFreqTable(df_New$length)
getFreqTable(df_New$verified)
getFreqTable(df_New$reason)
#see visual differences if any of good loans vs bad
boxplot(income~payment_status,data=df_New)
boxplot(amount~payment_status,data=df_New)
boxplot(debtIncRat~payment_status,data=df_New)
boxplot(openAcc~payment_status,data=df_New)
boxplot(totalAcc~payment_status,data=df_New)
boxplot(totalBal~payment_status,data=df_New)
boxplot(totalRevLim~payment_status,data=df_New)
boxplot(accOpen24~payment_status,data=df_New)
boxplot(bcOpen~payment_status,data=df_New)
boxplot(totalRevBal~payment_status,data=df_New)
